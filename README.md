## üß† Desafio T√©cnico ‚Äì RAG com NestJS + Ollama (Llama2)

Este projeto implementa um sistema de **RAG (Retrieval Augmented Generation)** utilizando as seguintes tecnologias:

- **NestJS** (Framework Node.js)
- **TypeORM** + **PostgreSQL** (para persist√™ncia de dados e vetores)
- **Ollama** (rodando localmente)
  - Modelo **Llama2** para gera√ß√£o de respostas.
  - Modelo **nomic-embed-text** para gerar embeddings.

O projeto oferece um CRUD completo para gerenciar documentos, embeddings e chatbots.

### üîÑ O Fluxo do RAG

O processo de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) funciona em duas fases:

1.  **Indexa√ß√£o (Preparation):**
    - Voc√™ faz upload de um documento.
    - O sistema quebra o documento em **chunks** (peda√ßos menores).
    - Gera **embeddings** (vetores num√©ricos) para cada chunk usando o **Ollama** local.
    - Salva os chunks e seus vetores no banco de dados.
2.  **Consulta (Generation):**
    - O usu√°rio envia uma pergunta (query).
    - √â feita uma **busca vetorial por similaridade** no banco para encontrar os chunks mais relevantes.
    - O contexto relevante (os chunks encontrados) √© formatado e enviado ao modelo **Llama2**.
    - O chatbot responde √† pergunta **exclusivamente com base** no conte√∫do do documento.

---

## ‚öôÔ∏è Pr√©-requisitos

Antes de rodar o projeto, voc√™ precisa ter instalado:

### ‚úî Node.js (vers√£o 18+)

[https://nodejs.org/](https://nodejs.org/)

### ‚úî PostgreSQL

√â necess√°rio criar o banco de dados antes de iniciar a API.

```sql
CREATE DATABASE technical_challenge;
```

### ‚úî Instalar o Ollama (Obrigat√≥rio)

Baixe e instale o Ollama para rodar modelos de LLM localmente.

[https://ollama.com/download](https://ollama.com/download)

Ap√≥s a instala√ß√£o, confirme a vers√£o:

```bash
ollama --version
# Exemplo de sa√≠da: ollama version 0.x.x
```

### üß† Instalar Modelos Necess√°rios no Ollama

O projeto utiliza dois modelos que devem ser baixados localmente:

| Modelo               | Fun√ß√£o                      | Comando de Instala√ß√£o          |
| :------------------- | :-------------------------- | :----------------------------- |
| **nomic-embed-text** | Gera√ß√£o de Embeddings       | `ollama pull nomic-embed-text` |
| **llama2**           | Gera√ß√£o de Chat (Respostas) | `ollama pull llama2`           |

Confirme a instala√ß√£o dos modelos:

```bash
ollama list
# Voc√™ deve ver:
# llama2:latest
# nomic-embed-text:latest
```

---

## ‚ñ∂Ô∏è Rodando o Projeto

Siga os passos para iniciar a API:

### 1\. Instalar depend√™ncias

```bash
npm install
```

### 2\. Configurar vari√°veis de ambiente

Crie um arquivo **`.env`** na raiz do projeto com as seguintes vari√°veis:

```env
DB_HOST=localhost
DB_PORT=5432
DB_USERNAME=postgres
DB_PASSWORD=sua_senha
DB_DATABASE=technical_challenge

PORT=3000
```

### 3\. Rodar a API

```bash
npm run start
```

A API estar√° dispon√≠vel no endere√ßo: **`http://localhost:3000/api`**

---

## üß™ Testando o Ollama Direto (Verifica√ß√£o de Sa√∫de)

Para garantir que o Ollama est√° funcionando na porta `11434` e os modelos est√£o carregados:

### Teste de Gera√ß√£o (`llama2`)

```bash
curl -X POST http://localhost:11434/api/generate \
 -d '{ "model": "llama2", "prompt": "Ol√°, tudo bem?", "stream": false }'
```

### Teste de Embedding (`nomic-embed-text`)

```bash
curl -X POST http://localhost:11434/api/embeddings \
 -d '{ "model": "nomic-embed-text", "prompt": "teste" }'
```

Se ambos retornarem dados em formato JSON, o Ollama est√° operacional.

---

## üß© Fluxo Completo do Sistema (Como Testar)

Use as rotas da API (`http://localhost:3000/api`) na ordem para testar o fluxo de RAG:

### 1\. Criar um Chatbot

- **M√©todo:** `POST`
- **Rota:** `/api/chatbots`

<!-- end list -->

```json
{
  "name": "Meu chatbot",
  "description": "Chatbot para testes"
}
```

### 2\. Fazer Upload de um Documento

- **M√©todo:** `POST`
- **Rota:** `/api/documents`
  - _(O sistema salva o arquivo no banco e extrai o texto.)_

### 3\. Gerar Embeddings

- **M√©todo:** `POST`
- **Rota:** `/api/embeddings/:documentId`

Este passo ir√°:

- Dividir o documento em **chunks**.
- Gerar o embedding de cada chunk usando **`nomic-embed-text`**.
- Salvar os vetores no banco de dados.

### 4\. Fazer uma Pergunta usando RAG

- **M√©todo:** `POST`
- **Rota:** `/api/chat/:chatbotId`

<!-- end list -->

```json
{
  "documentId": "id-do-documento",
  "question": "Qual √© o assunto principal do documento?"
}
```

A API executar√° o fluxo de RAG completo:

- Buscar os chunks mais parecidos (similaridade vetorial).
- Montar o prompt com a pergunta e o contexto.
- Enviar para o **Llama2** local.
- Retornar a resposta final.

---

## üöÄ Tecnologias Utilizadas

- **NestJS**
- **TypeORM**
- **PostgreSQL**
- **Ollama** (local)
- **Llama2**
- **nomic-embed-text**
- **RAG (Retrieval Augmented Generation)**
- Processamento e chunking de documentos

<!-- end list -->

```

```
